{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDVe/Kt/h8t/3TaSvxSUgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hearthatu/Basic-Python/blob/main/49%EC%B0%A8%EC%8B%9C%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81%EC%9D%84%20%ED%95%B4%EC%95%BC%ED%95%98%EB%8A%94%20%EC%9D%B4%EC%9C%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 스케일링 해야하는 이유\n",
        "\n",
        "- 데이터 스케일링이란?\n",
        "\n",
        "> 데이터 전처리 과정\n",
        "\n",
        "> 데이터 특성의 범위 또는 분포를 똑같이 만드는 작업\n",
        "\n",
        "- StandardScaler\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 모든 특성을 평균을 0으로, 표준편차를 1로 변환한다\n",
        "# 이상치에 민감하다.\n",
        "```\n",
        "\n",
        "- MinMaxScaler\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 특성 중 가장 작은 값을 0, 가장 큰 값을 1로 변환하고 0~1사이의 값을 만들어 이상치에 매우 민\n",
        "\n",
        "```\n",
        "\n",
        "- RobustScaler\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "# 중앙값과 사분위 값을 사용하고 중앙값을 빼고, 사분위 값으로 나눈다.\n",
        "# 이상치의 영향을 최소화할 수 있다.\n",
        "```\n"
      ],
      "metadata": {
        "id": "GTQOaxtVuUcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "fish = pd.read_csc(\"fish.csv\")\n",
        "fish\n",
        "\n",
        "bream = fish[fish[\"class\"]==1]\n",
        "smelt = fish[fish[\"class\"]==0]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(bream[\"length\"], bream[\"weight\"]])\n",
        "plt.scatter(smelt[\"length\"], smelt[\"weight\"])\n",
        "plt.scatter(25, 150, makrker = \"^\")\n",
        "\n",
        "\n",
        "data = fish [[\"length\", \"weight\"]].to_numpy()\n",
        "target = fish[\"class\"].to_numpy()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(train_input, train_target)\n",
        "knn.score(test_input, test_target)\n",
        "\n",
        "knn.predic([[25, 150]])\n",
        "\n",
        "# 데이터 표준화\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "mean = np.mean(train_input, axis=0) # 행 방향\n",
        "std = np.std(train_input, axis=0)\n",
        "\n",
        "train_scaled = (train_input - mean) / std\n",
        "test_scaled = (test_input - mean) / std\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(train_scaled, train_target)\n",
        "knn.score(test_scaled, test_target)\n",
        "\n",
        "new = ([25, 150] - mean) / std\n",
        "new\n",
        "\n",
        "knn.predic([new])"
      ],
      "metadata": {
        "id": "V06nQ-Pkvf7V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}